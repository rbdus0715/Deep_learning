{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cd1b10",
   "metadata": {},
   "source": [
    "# Conv2d\n",
    "- in_channels (int) – Number of channels in the input image\n",
    "\n",
    "- out_channels (int) – Number of channels produced by the convolution\n",
    "\n",
    "- kernel_size (int or tuple) – Size of the convolving kernel\n",
    "\n",
    "- stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "\n",
    "- padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n",
    "\n",
    "- padding_mode (string, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n",
    "\n",
    "- dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\n",
    "\n",
    "- groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\n",
    "\n",
    "- bias (bool, optional) – If True, adds a learnable bias to the output. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587001c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Conv2d(\n",
    "#     in_channels, : input 채널 몇장\n",
    "#     out_channels, : 필터 몇장\n",
    "#     kernel_size, : 커널 사이즈 (괄호로 싸서 표현 가능 x by y)\n",
    "#     stride=1, \n",
    "#     padding=0,\n",
    "#     dilation=1,\n",
    "#     groups=1,\n",
    "#     bias=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574d56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력의 형태\n",
    "# conv = nn.Conv2d(1,1,3)\n",
    "# output = conv(input) <- 여기에서 input의 형태는 torch.Tensor 타입\n",
    "# input shape : ( N * C * H * W ) \n",
    "# => (batch_size, channel, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d3223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 크기\n",
    "# output size : (input size - filter size + (2*padding))/stride + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d79ca64",
   "metadata": {},
   "source": [
    "# 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3cf958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a1c3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 55, 55])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 1 : input image 크기가 227*227, 필터는 11*11, stride 4, padding 0 \n",
    "conv = nn.Conv2d(1, 1, 11, stride=4, padding=0)\n",
    "inputs = torch.Tensor(1, 1, 227, 227)\n",
    "out = conv(inputs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9de912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 29, 29])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 2\n",
    "conv = nn.Conv2d(1, 1, 7, stride=2, padding=0)\n",
    "inputs = torch.Tensor(1, 1, 64, 64)\n",
    "out = conv(inputs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1bb05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 3\n",
    "conv = nn.Conv2d(1, 1, 5, stride=1, padding=2)\n",
    "inputs = torch.Tensor(1, 1, 32, 32)\n",
    "out = conv(inputs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d498f7",
   "metadata": {},
   "source": [
    "# MaxPool2d\n",
    "\n",
    "- kernel_size – the size of the window to take a max over\n",
    "\n",
    "- stride – the stride of the window. Default value is kernel_size\n",
    "\n",
    "- padding – implicit zero padding to be added on both sides\n",
    "\n",
    "- dilation – a parameter that controls the stride of elements in the window\n",
    "\n",
    "- return_indices – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later\n",
    "\n",
    "- ceil_mode – when True, will use ceil instead of floor to compute the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41eeb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MaxPool2d(\n",
    "#     kernel_size, \n",
    "#     stride=None,\n",
    "#     padding=0,\n",
    "#     dilation=1,\n",
    "#     return_indices=False,\n",
    "#     ceil_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a14871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 24, 24]) torch.Size([1, 5, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.Tensor(1, 1, 28, 28)\n",
    "conv1 = nn.Conv2d(1, 5, 5) # input 1, output 5\n",
    "pool = nn.MaxPool2d(2)\n",
    "out = conv1(inputs)\n",
    "out2 = pool(out)\n",
    "print(out.size(), out2.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
